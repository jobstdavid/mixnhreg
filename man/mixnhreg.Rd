% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mixnhreg.R
\name{mixnhreg}
\alias{mixnhreg}
\title{Mixtures of Non-Homogeneous Linear Regression Models}
\usage{
mixnhreg(
  formula,
  scale.formula,
  weight.formula,
  data,
  na.action = na.omit,
  family = mixnorm(),
  loss = "nll",
  control = control_optim(),
  ...
)
}
\arguments{
\item{formula}{a formula object, with the response on the left of the \code{~} operator followed by the mixture components separated by an \code{|} operator,
where the predictors related to the location parameter of each mixture component are split up by the \code{+} operator.}

\item{scale.formula}{a formula object starting with the \code{~} operator followed by the mixture components separated by an \code{|} operator,
where the predictors related to the scale parameter of each mixture component are split up by the \code{+} operator.}

\item{weight.formula}{a formula object starting with the \code{~} operator followed by the mixture components separated by an \code{|} operator,
where the predictors related to the weight parameter of each mixture component are split up by the \code{+} operator.}

\item{data}{a data frame containing the variables occurring in the formulas.}

\item{na.action}{a function which indicates what should happen when the data contains \code{NA}s. Default: \code{na.omit}.}

\item{family}{a \code{mixnhreg.family} object, specifying details of the modeled distribution. Default: \code{\link{mixnorm}()}.}

\item{loss}{loss function used for optimization, which is either negative log-likelihood "\code{nll}" (default) or
continuous ranked probability score "\code{crps}".}

\item{control}{either \code{\link{control_optim}()} (default) for estimation via \code{\link{optim}} or \code{\link{control_boost}()} for estimation via gradient-boosting.}

\item{...}{unused.}
}
\value{
An object of class \code{mixnhreg}.
}
\description{
This is the main function of \pkg{mixnhreg} for estimating mixtures of non-homogeneous linear regression models.
}
\examples{
# load data
data("station")

# fit mixture normal distribution with two components via BFGS
(fit_optim <- mixnhreg(formula = obs ~ sin1 + cos1 + temp_mean | temp_ctrl,
                       scale.formula = ~ sin1 + cos1 + log(temp_sd) | 1,
                       weight.formula = ~ sin1 + cos1 | 1,
                       data = station,
                       control = control_optim()))

# fit mixture normal distribution with two components via gradient-boosting
(fit_boost <- mixnhreg(formula = obs ~ sin1 + cos1 + temp_mean | temp_ctrl,
                       scale.formula = ~ sin1 + cos1 + log(temp_sd) | 1,
                       weight.formula = ~ sin1 + cos1 | 1,
                       data = station,
                       control = control_boost(mstop = "cv")))

# model summary
summary(fit_optim)
summary(fit_boost)

# parameter predictions
par_optim <- predict(fit_optim, type = "parameter")
par_boost <- predict(fit_boost, type = "parameter")

# get observations and mean forecasts
obs <- na.omit(station$obs)
mean_optim <- rowSums(par_optim$location * par_optim$weight)
mean_boost <- rowSums(par_boost$location * par_boost$weight)

# residual plot
plot(obs - mean_optim,
     xlab = "Index",
     ylab = "Observation - Mean",
     pch = 19,
     col = adjustcolor("red", alpha = 0.5))
points(obs - mean_boost,
       pch = 19,
       col = adjustcolor("steelblue", alpha = 0.25))
legend("bottomright",
       legend = c("optim", "boost"),
       pch = 19,
       col = c("red", "steelblue"))
grid()


}
\references{
Hepp, T., J. Zierk, and E. Bergherr (2023). "Component-wise boosting for mixture distributional regression models".
In: Proceedings of the 37th International Workshop on Statistical Modelling. url: https://iwsm2023.statistik.tu- dortmund.de/.

Jobst, D. (2024). "Gradient-Boosted Mixture Regression Models for Postprocessing Ensemble Weather Forecasts". doi: <https://doi.org/10.48550/arXiv.2412.09583>.

Messner, J. W., G. J. Mayr, and A. Zeileis (2017). "Nonhomogeneous Boosting for Predictor Selection in Ensemble Postprocessing".
In: Monthly Weather Review 145.1, pp. 137–147. doi: <https://doi.org/10.1175/mwr-d-16-0088.1>.

Thomas, J. et al. (2017). "Gradient boosting for distributional regression: faster tuning and improved variable selection via noncyclical updates".
In: Statistics and Computing 28.3, pp. 673–687. doi: <https://doi.org/10.1007/s11222-017-9754-6>.
}
